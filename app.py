import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EllipticEnvelope
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import pickle
import json
import io
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏–≥–º–∞-—Ñ–∞–∑—ã",
    page_icon="üî¨",
    layout="wide"
)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üî¨ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∏–Ω–µ—Ç–∏–∫–∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–∏–≥–º–∞-—Ñ–∞–∑—ã –≤ —Å—Ç–∞–ª–∏ 12–•18–ù12–¢")
st.markdown("""
### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Å–∏–≥–º–∞-—Ñ–∞–∑—ã, –≤—Ä–µ–º–µ–Ω–∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –∏ –Ω–æ–º–µ—Ä—É –∑–µ—Ä–Ω–∞
""")

class ComplexDataParser:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–ª–æ–∂–Ω—ã—Ö Excel —Ñ–∞–π–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–∏–≥–º–∞-—Ñ–∞–∑—ã"""
    
    @staticmethod
    def parse_complex_excel(file_path):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–ª–æ–∂–Ω–æ–≥–æ Excel —Ñ–∞–π–ª–∞ —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            df = pd.read_excel(file_path, sheet_name=0, header=None)
            
            results = []
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º —Å –¥–∞–Ω–Ω—ã–º–∏ (–Ω–∞—á–∏–Ω–∞—è —Å–æ —Å—Ç—Ä–æ–∫–∏ 2, —Ç–∞–∫ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ 1 - –∑–∞–≥–æ–ª–æ–≤–æ–∫)
            for i in range(2, len(df)):
                row = df.iloc[i]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–µ—Ä–≤—ã—Ö 4 –∫–æ–ª–æ–Ω–∫–∞—Ö
                if pd.notna(row[0]) and pd.notna(row[1]) and pd.notna(row[2]) and pd.notna(row[3]):
                    try:
                        G = float(row[0])
                        T = float(row[1])
                        t = float(row[2])
                        f_exp = float(row[3])
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
                        if (G in [3, 5, 8, 9, 10] and 
                            T in [600, 650, 700] and 
                            t in [2000, 4000, 6000, 8000] and
                            0 <= f_exp <= 10):
                            
                            results.append({
                                'G': G,
                                'T': T, 
                                't': t,
                                'f_exp (%)': f_exp
                            })
                    except (ValueError, TypeError):
                        continue
            
            return pd.DataFrame(results)
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–∞: {e}")

    @staticmethod
    def extract_all_data(uploaded_file):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with open("temp_file.xlsx", "wb") as f:
                f.write(uploaded_file.getvalue())
            
            # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
            data = ComplexDataParser.parse_complex_excel("temp_file.xlsx")
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            import os
            if os.path.exists("temp_file.xlsx"):
                os.remove("temp_file.xlsx")
                
            # –í–û–ó–í–†–ê–©–ê–ï–ú –¢–û–õ–¨–ö–û –ï–°–õ–ò –ï–°–¢–¨ –î–ê–ù–ù–´–ï
            if data is not None and len(data) > 0:
                return data
            else:
                return None
                
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

class DataValidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def normalize_column_names(df):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        column_mapping = {
            '–ù–æ–º–µ—Ä_–∑–µ—Ä–Ω–∞': 'G', '–ù–æ–º–µ—Ä –∑–µ—Ä–Ω–∞': 'G', '–ó–µ—Ä–Ω–æ': 'G',
            '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞': 'T', '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞_C': 'T', '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ ¬∞C': 'T',
            '–í—Ä–µ–º—è': 't', '–í—Ä–µ–º—è_—á': 't', '–í—Ä–µ–º—è, —á': 't',
            '–°–∏–≥–º–∞_—Ñ–∞–∑–∞': 'f_exp (%)', '–°–∏–≥–º–∞-—Ñ–∞–∑–∞': 'f_exp (%)', 
            '–°–∏–≥–º–∞_—Ñ–∞–∑–∞_%': 'f_exp (%)', '–°–∏–≥–º–∞ —Ñ–∞–∑–∞': 'f_exp (%)',
            'Grain': 'G', 'Grain_number': 'G', 'Grain size': 'G',
            'Temperature': 'T', 'Temp': 'T', 'Temperature_C': 'T',
            'Time': 't', 'Time_h': 't', 'Hours': 't',
            'Sigma_phase': 'f_exp (%)', 'Sigma': 'f_exp (%)', 
            'Sigma_%': 'f_exp (%)', 'f_exp': 'f_exp (%)'
        }
        
        df_normalized = df.copy()
        new_columns = {}
        
        for col in df.columns:
            col_clean = str(col).strip()
            if col_clean in column_mapping:
                new_columns[col] = column_mapping[col_clean]
            else:
                new_columns[col] = col_clean
        
        df_normalized.columns = [new_columns[col] for col in df.columns]
        return df_normalized
    
    @staticmethod
    def validate_data(df):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö"""
        required_columns = ['G', 'T', 't', 'f_exp (%)']
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return False, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"
        
        try:
            df['G'] = pd.to_numeric(df['G'], errors='coerce')
            df['T'] = pd.to_numeric(df['T'], errors='coerce')
            df['t'] = pd.to_numeric(df['t'], errors='coerce')
            df['f_exp (%)'] = pd.to_numeric(df['f_exp (%)'], errors='coerce')
        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö: {e}"
        
        if df[required_columns].isna().any().any():
            return False, "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—É—Å—Ç—ã–µ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö"
        
        if (df['G'] < -3).any() or (df['G'] > 14).any():
            return False, "–ù–æ–º–µ—Ä –∑–µ—Ä–Ω–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç -3 –¥–æ 14"
        
        if (df['T'] < 500).any() or (df['T'] > 1000).any():
            st.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ typical –¥–∏–∞–ø–∞–∑–æ–Ω 500-1000¬∞C")
        
        if (df['f_exp (%)'] < 0).any() or (df['f_exp (%)'] > 50).any():
            st.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å–∏–≥–º–∞-—Ñ–∞–∑—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ typical –¥–∏–∞–ø–∞–∑–æ–Ω 0-50%")
        
        DataValidator.validate_time_range(df['t'])
        
        return True, "–î–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã"
    
    @staticmethod
    def validate_time_range(t_values):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏"""
        max_time = 500000
        if (t_values > max_time).any():
            st.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ —Å–≤—ã—à–µ {max_time} —á–∞—Å–æ–≤")
        return True

class GrainSizeConverter:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–æ–º–µ—Ä–∞ –∑–µ—Ä–Ω–∞ –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ –ì–û–°–¢ 5639-82"""
    
    GRAIN_DATA = {
        -3: {'area_mm2': 1.000, 'diameter_mm': 1.000, 'conditional_diameter_mm': 0.875, 'grains_per_mm2': 1.0},
        -2: {'area_mm2': 0.500, 'diameter_mm': 0.707, 'conditional_diameter_mm': 0.650, 'grains_per_mm2': 2.8},
        -1: {'area_mm2': 0.250, 'diameter_mm': 0.500, 'conditional_diameter_mm': 0.444, 'grains_per_mm2': 8.0},
        0:  {'area_mm2': 0.125, 'diameter_mm': 0.353, 'conditional_diameter_mm': 0.313, 'grains_per_mm2': 22.6},
        1:  {'area_mm2': 0.0625, 'diameter_mm': 0.250, 'conditional_diameter_mm': 0.222, 'grains_per_mm2': 64.0},
        2:  {'area_mm2': 0.0312, 'diameter_mm': 0.177, 'conditional_diameter_mm': 0.157, 'grains_per_mm2': 181.0},
        3:  {'area_mm2': 0.0156, 'diameter_mm': 0.125, 'conditional_diameter_mm': 0.111, 'grains_per_mm2': 512.0},
        4:  {'area_mm2': 0.00781, 'diameter_mm': 0.088, 'conditional_diameter_mm': 0.0783, 'grains_per_mm2': 1448.0},
        5:  {'area_mm2': 0.00390, 'diameter_mm': 0.062, 'conditional_diameter_mm': 0.0553, 'grains_per_mm2': 4096.0},
        6:  {'area_mm2': 0.00195, 'diameter_mm': 0.044, 'conditional_diameter_mm': 0.0391, 'grains_per_mm2': 11585.0},
        7:  {'area_mm2': 0.00098, 'diameter_mm': 0.031, 'conditional_diameter_mm': 0.0267, 'grains_per_mm2': 32768.0},
        8:  {'area_mm2': 0.00049, 'diameter_mm': 0.022, 'conditional_diameter_mm': 0.0196, 'grains_per_mm2': 92682.0},
        9:  {'area_mm2': 0.000244, 'diameter_mm': 0.015, 'conditional_diameter_mm': 0.0138, 'grains_per_mm2': 262144.0},
        10: {'area_mm2': 0.000122, 'diameter_mm': 0.011, 'conditional_diameter_mm': 0.0099, 'grains_per_mm2': 741485.0},
        11: {'area_mm2': 0.000061, 'diameter_mm': 0.0079, 'conditional_diameter_mm': 0.0069, 'grains_per_mm2': 2097152.0},
        12: {'area_mm2': 0.000030, 'diameter_mm': 0.0056, 'conditional_diameter_mm': 0.0049, 'grains_per_mm2': 5931008.0},
        13: {'area_mm2': 0.000015, 'diameter_mm': 0.0039, 'conditional_diameter_mm': 0.0032, 'grains_per_mm2': 16777216.0},
        14: {'area_mm2': 0.000008, 'diameter_mm': 0.0027, 'conditional_diameter_mm': 0.0027, 'grains_per_mm2': 47449064.0}
    }
    
    @classmethod
    def grain_number_to_area(cls, grain_number):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –∑–µ—Ä–Ω–∞ –≤ —Å—Ä–µ–¥–Ω—é—é –ø–ª–æ—â–∞–¥—å —Å–µ—á–µ–Ω–∏—è (–º–º¬≤)"""
        data = cls.GRAIN_DATA.get(grain_number)
        if data:
            return data['area_mm2']
        else:
            numbers = sorted(cls.GRAIN_DATA.keys())
            if grain_number < numbers[0]:
                return cls.GRAIN_DATA[numbers[0]]['area_mm2']
            elif grain_number > numbers[-1]:
                return cls.GRAIN_DATA[numbers[-1]]['area_mm2']
            else:
                lower = max([n for n in numbers if n <= grain_number])
                upper = min([n for n in numbers if n >= grain_number])
                if lower == upper:
                    return cls.GRAIN_DATA[lower]['area_mm2']
                log_area_lower = np.log(cls.GRAIN_DATA[lower]['area_mm2'])
                log_area_upper = np.log(cls.GRAIN_DATA[upper]['area_mm2'])
                fraction = (grain_number - lower) / (upper - lower)
                log_area = log_area_lower + fraction * (log_area_upper - log_area_lower)
                return np.exp(log_area)
    
    @classmethod
    def grain_number_to_diameter(cls, grain_number, use_conditional=True):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –∑–µ—Ä–Ω–∞ –≤ –¥–∏–∞–º–µ—Ç—Ä (–º–º)"""
        data = cls.GRAIN_DATA.get(grain_number)
        if data:
            return data['conditional_diameter_mm'] if use_conditional else data['diameter_mm']
        else:
            numbers = sorted(cls.GRAIN_DATA.keys())
            if grain_number < numbers[0]:
                return cls.GRAIN_DATA[numbers[0]]['conditional_diameter_mm'] if use_conditional else cls.GRAIN_DATA[numbers[0]]['diameter_mm']
            elif grain_number > numbers[-1]:
                return cls.GRAIN_DATA[numbers[-1]]['conditional_diameter_mm'] if use_conditional else cls.GRAIN_DATA[numbers[-1]]['diameter_mm']
            else:
                lower = max([n for n in numbers if n <= grain_number])
                upper = min([n for n in numbers if n >= grain_number])
                if lower == upper:
                    return cls.GRAIN_DATA[lower]['conditional_diameter_mm'] if use_conditional else cls.GRAIN_DATA[lower]['diameter_mm']
                diam_lower = cls.GRAIN_DATA[lower]['conditional_diameter_mm'] if use_conditional else cls.GRAIN_DATA[lower]['diameter_mm']
                diam_upper = cls.GRAIN_DATA[upper]['conditional_diameter_mm'] if use_conditional else cls.GRAIN_DATA[upper]['diameter_mm']
                fraction = (grain_number - lower) / (upper - lower)
                return diam_lower + fraction * (diam_upper - diam_lower)
    
    @classmethod
    def calculate_grain_boundary_density(cls, grain_number):
        """–†–∞—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≥—Ä–∞–Ω–∏—Ü –∑–µ—Ä–µ–Ω (–º–º¬≤/–º–º¬≥)"""
        d = cls.grain_number_to_diameter(grain_number, use_conditional=True)
        Sv = 3.0 / (d / 2.0)
        return Sv
    
    @classmethod
    def calculate_activation_energy_factor(cls, grain_number):
        """–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∑–µ—Ä–Ω–∞ –Ω–∞ —ç–Ω–µ—Ä–≥–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"""
        ref_grain = 5
        Sv_ref = cls.calculate_grain_boundary_density(ref_grain)
        Sv_current = cls.calculate_grain_boundary_density(grain_number)
        return Sv_current / Sv_ref

class SigmaPhaseAnalyzer:
    def __init__(self):
        self.params = None
        self.R2 = None
        self.rmse = None
        self.mape = None
        self.model_type = None
        self.final_formula = ""
        
    def fit_model(self, data, model_type="avrami_saturation"):
        """–ü–æ–¥–≥–æ–Ω–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        try:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—á–µ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 580-630¬∞C
            working_data = data[(data['T'] >= 580) & (data['T'] <= 630)].copy()
            
            # –ï—Å–ª–∏ –≤ —Ä–∞–±–æ—á–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω–æ —Å –≤–µ—Å–∞–º–∏
            if len(working_data) < 8:
                weights = np.ones(len(data))
                # –î–∞–µ–º –±–æ–ª—å—à–∏–π –≤–µ—Å —Ç–æ—á–∫–∞–º –≤ —Ä–∞–±–æ—á–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
                weights[(data['T'] >= 580) & (data['T'] <= 630)] = 3.0
                weights[(data['T'] >= 550) & (data['T'] < 580)] = 1.5
                weights[(data['T'] > 630) & (data['T'] <= 700)] = 1.5
                used_data = data
            else:
                weights = np.ones(len(working_data))
                used_data = working_data
            
            G = used_data['G'].values
            T = used_data['T'].values + 273.15  # –≤ –ö–µ–ª—å–≤–∏–Ω—ã
            t = used_data['t'].values
            f_exp = used_data['f_exp (%)'].values
            
            self.model_type = model_type
            
            if model_type == "avrami_saturation":
                success = self._fit_avrami_model(G, T, t, f_exp, weights)
            elif model_type == "power_law":
                success = self._fit_power_law_model(G, T, t, f_exp, weights)
            elif model_type == "logistic":
                success = self._fit_logistic_model(G, T, t, f_exp, weights)
            elif model_type == "ensemble":
                success = self._fit_ensemble_model(G, T, t, f_exp, weights)
            else:
                st.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
                return False
                
            if success:
                self._generate_final_formula()
                
            return success
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ–Ω–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def _fit_avrami_model(self, G, T, t, f_exp, weights):
        """–ú–æ–¥–µ–ª—å –ê–≤—Ä–∞–º–∏ —Å –Ω–∞—Å—ã—â–µ–Ω–∏–µ–º –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        # –ù–∞—á–∞–ª—å–Ω—ã–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º —Ñ–∏–∑–∏–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞
        initial_guess = [12.0, 1e12, 250000, 1.2, 0.15]  # f_max, K0, Q, n, alpha
        
        # –ì—Ä–∞–Ω–∏—Ü—ã —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
        bounds = (
            [5.0, 1e8, 200000, 0.8, 0.05],   # –Ω–∏–∂–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
            [25.0, 1e16, 350000, 2.5, 0.3]    # –≤–µ—Ä—Ö–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã
        )
        
        def model(params, G, T, t):
            f_max, K0, Q, n, alpha = params
            R = 8.314
            
            # –≠—Ñ—Ñ–µ–∫—Ç —Ä–∞–∑–º–µ—Ä–∞ –∑–µ—Ä–Ω–∞
            grain_effect = 1 + alpha * (G - 8)
            
            # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º —ç–Ω–µ—Ä–≥–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
            K = K0 * np.exp(-Q / (R * T)) * grain_effect
            
            # –ú–æ–¥–µ–ª—å –ê–≤—Ä–∞–º–∏
            return f_max * (1 - np.exp(-K * (t ** n)))
        
        try:
            self.params, _ = curve_fit(
                lambda x, f_max, K0, Q, n, alpha: model([f_max, K0, Q, n, alpha], G, T, t),
                np.arange(len(G)), f_exp,
                p0=initial_guess,
                bounds=bounds,
                maxfev=10000,
                sigma=1.0/weights  # –≤–µ—Å–∞ –¥–ª—è —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö
            )
            
            f_pred = model(self.params, G, T, t)
            self.R2 = r2_score(f_exp, f_pred)
            self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
            self.mape = np.mean(np.abs((f_exp - f_pred) / np.maximum(f_exp, 0.1))) * 100
            
            return True
            
        except Exception as e:
            st.warning(f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ê–≤—Ä–∞–º–∏ –Ω–µ —Å–æ—à–ª–∞—Å—å, –ø—Ä–æ–±—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
            return self._fit_simplified_avrami(G, T, t, f_exp, weights)
    
    def _fit_simplified_avrami(self, G, T, t, f_exp, weights):
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ê–≤—Ä–∞–º–∏ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        initial_guess = [10.0, 1e10, 220000, 0.1]  # f_max, K0, Q, alpha
        
        bounds = (
            [5.0, 1e8, 180000, 0.05],
            [20.0, 1e14, 280000, 0.2]
        )
        
        def model(params, G, T, t):
            f_max, K0, Q, alpha = params
            R = 8.314
            grain_effect = 1 + alpha * (G - 8)
            K = K0 * np.exp(-Q / (R * T)) * grain_effect
            # –§–∏–∫—Å–∏—Ä—É–µ–º n = 1 –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
            return f_max * (1 - np.exp(-K * t))
        
        self.params, _ = curve_fit(
            lambda x, f_max, K0, Q, alpha: model([f_max, K0, Q, alpha], G, T, t),
            np.arange(len(G)), f_exp,
            p0=initial_guess,
            bounds=bounds,
            maxfev=5000,
            sigma=1.0/weights
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π n = 1 –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        self.params = np.append(self.params, 1.0)
        
        f_pred = model(self.params[:-1], G, T, t)
        self.R2 = r2_score(f_exp, f_pred)
        self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
        self.mape = np.mean(np.abs((f_exp - f_pred) / np.maximum(f_exp, 0.1))) * 100
        
        return True
    
    def _fit_power_law_model(self, G, T, t, f_exp, weights):
        """–°—Ç–µ–ø–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        initial_guess = [2.0, 0.5, -18000, 0.7, 0.08]  # A, B, C, D, E
        
        bounds = (
            [0.1, 0.0, -30000, 0.3, 0.02],
            [10.0, 2.0, -12000, 1.5, 0.2]
        )
        
        def model(params, G, T, t):
            A, B, C, D, E = params
            R = 8.314
            # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (–æ–±—Ä–∞—Ç–Ω–∞—è)
            temp_effect = np.exp(C / (R * T))
            time_effect = t ** D
            grain_effect = 1 + E * (G - 8)
            return A * temp_effect * time_effect * grain_effect + B
        
        self.params, _ = curve_fit(
            lambda x, A, B, C, D, E: model([A, B, C, D, E], G, T, t),
            np.arange(len(G)), f_exp,
            p0=initial_guess,
            bounds=bounds,
            maxfev=5000,
            sigma=1.0/weights
        )
        
        f_pred = model(self.params, G, T, t)
        self.R2 = r2_score(f_exp, f_pred)
        self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
        self.mape = np.mean(np.abs((f_exp - f_pred) / np.maximum(f_exp, 0.1))) * 100
        return True
    
    def _fit_logistic_model(self, G, T, t, f_exp, weights):
        """–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å —Å –Ω–∞—Å—ã—â–µ–Ω–∏–µ–º"""
        initial_guess = [15.0, 1e-6, 2000, 0.12, -15000]  # f_max, k, t0, alpha, beta
        
        bounds = (
            [8.0, 1e-8, 500, 0.05, -25000],
            [30.0, 1e-4, 5000, 0.25, -8000]
        )
        
        def model(params, G, T, t):
            f_max, k, t0, alpha, beta = params
            R = 8.314
            temp_factor = np.exp(beta / (R * T))
            grain_factor = 1 + alpha * (G - 8)
            rate = k * temp_factor * grain_factor
            return f_max / (1 + np.exp(-rate * (t - t0)))
        
        self.params, _ = curve_fit(
            lambda x, f_max, k, t0, alpha, beta: model([f_max, k, t0, alpha, beta], G, T, t),
            np.arange(len(G)), f_exp,
            p0=initial_guess,
            bounds=bounds,
            maxfev=5000,
            sigma=1.0/weights
        )
        
        f_pred = model(self.params, G, T, t)
        self.R2 = r2_score(f_exp, f_pred)
        self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
        self.mape = np.mean(np.abs((f_exp - f_pred) / np.maximum(f_exp, 0.1))) * 100
        return True
    
    def _fit_ensemble_model(self, G, T, t, f_exp, weights):
        """–ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ —Ä–∞–±–æ—á–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω"""
        initial_guess = [10.0, 1e10, 220000, 1.0, 0.1, 0.5, -15000]
        
        bounds = (
            [5.0, 1e8, 180000, 0.5, 0.05, 0.1, -25000],
            [20.0, 1e14, 280000, 1.8, 0.2, 2.0, -8000]
        )
        
        def model(params, G, T, t):
            f_max, K0, Q, n, alpha, w, beta = params
            R = 8.314
            
            # –ê–≤—Ä–∞–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–æ—Å–Ω–æ–≤–Ω–æ–π)
            grain_effect_avrami = 1 + alpha * (G - 8)
            K_avrami = K0 * np.exp(-Q / (R * T)) * grain_effect_avrami
            f_avrami = f_max * (1 - np.exp(-K_avrami * (t ** n)))
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —É—á–µ—Ç–∞ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–µ–π
            temp_effect_power = np.exp(beta / (R * T))
            f_power = w * temp_effect_power * (t ** 0.3) * (1 + 0.03 * (G - 8))
            
            return f_avrami + f_power
        
        self.params, _ = curve_fit(
            lambda x, f_max, K0, Q, n, alpha, w, beta: model([f_max, K0, Q, n, alpha, w, beta], G, T, t),
            np.arange(len(G)), f_exp,
            p0=initial_guess,
            bounds=bounds,
            maxfev=15000,
            sigma=1.0/weights
        )
        
        f_pred = model(self.params, G, T, t)
        self.R2 = r2_score(f_exp, f_pred)
        self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
        self.mape = np.mean(np.abs((f_exp - f_pred) / np.maximum(f_exp, 0.1))) * 100
        return True

    def _generate_final_formula(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–∏—Ç–∞–µ–º–æ–π —Ñ–æ—Ä–º—É–ª—ã –º–æ–¥–µ–ª–∏"""
        if self.params is None:
            self.final_formula = "–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞"
            return
            
        if self.model_type == "avrami_saturation":
            f_max, K0, Q, n, alpha = self.params
            self.final_formula = f"""
**–ú–æ–¥–µ–ª—å –ê–≤—Ä–∞–º–∏ —Å –Ω–∞—Å—ã—â–µ–Ω–∏–µ–º:**
            f(G, T, t) = {f_max:.3f} √ó [1 - exp(-K √ó t^{n:.3f})]
K = {K0:.3e} √ó exp(-{Q/1000:.1f} –∫–î–∂/–º–æ–ª—å / (R √ó T)) √ó [1 + {alpha:.3f} √ó (G - 8)]
            """
        elif self.model_type == "power_law":
            A, B, C, D, E = self.params
            self.final_formula = f"""
**–°—Ç–µ–ø–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å:**
            f(G, T, t) = {A:.3f} √ó exp({C:.0f} / (R √ó T)) √ó t^{D:.3f} √ó [1 + {E:.3f} √ó (G - 8)] + {B:.3f}
            """
        elif self.model_type == "logistic":
            f_max, k, t0, alpha, beta = self.params
            self.final_formula = f"""
**–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å:**
            f(G, T, t) = {f_max:.3f} / [1 + exp(-k √ó (t - {t0:.0f}))]
k = {k:.3e} √ó exp({beta:.0f} / (R √ó T)) √ó [1 + {alpha:.3f} √ó (G - 8)]
            """
        elif self.model_type == "ensemble":
            f_max, K0, Q, n, alpha, w, beta = self.params
            self.final_formula = f"""
**–ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å:**
            f(G, T, t) = f_avrami + f_power

f_avrami = {f_max:.3f} √ó [1 - exp(-K_avrami √ó t^{n:.3f})]
K_avrami = {K0:.3e} √ó exp(-{Q/1000:.1f} –∫–î–∂/–º–æ–ª—å / (R √ó T)) √ó [1 + {alpha:.3f} √ó (G - 8)]

f_power = {w:.3f} √ó exp({beta:.0f} / (R √ó T)) √ó t^0.3 √ó [1 + 0.03 √ó (G - 8)]
            """
      
        self.final_formula += "\n**R = 8.314 –î–∂/(–º–æ–ª—å¬∑–ö) - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–∞–∑–æ–≤–∞—è –ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è**\n**T - —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –ö–µ–ª—å–≤–∏–Ω–∞—Ö (T[¬∞C] + 273.15)**"
    
    def predict_temperature(self, G, sigma_percent, t):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
        if self.params is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        sigma = sigma_percent
        
        # –ë–∏—Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Ä–∞–±–æ—á–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ 580-630¬∞C
        T_min, T_max = 580, 630
        
        for i in range(50):  # —É–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            T_mid = (T_min + T_max) / 2
            f_pred = self._evaluate_model(G, T_mid, t)
            
            if abs(f_pred - sigma) < 0.5:  # –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
                return T_mid
            
            if f_pred < sigma:
                T_min = T_mid
            else:
                T_max = T_mid
        
        # –ï—Å–ª–∏ –Ω–µ —Å–æ—à–ª–æ—Å—å –≤ —Ä–∞–±–æ—á–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ, —Ä–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∏—Å–∫
        final_T = (T_min + T_max) / 2
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        if final_T < 550:
            st.warning("‚ö†Ô∏è –†–∞—Å—á–µ—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∏–∂–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–µ–ª–∞ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–∏–≥–º–∞-—Ñ–∞–∑—ã (550¬∞C)")
            return 550
        elif final_T > 700:
            st.warning("‚ö†Ô∏è –†–∞—Å—á–µ—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤—ã—à–µ —Ç–∏–ø–∏—á–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è —Å—Ç–∞–ª–∏ 12–•18–ù12–¢")
            return 700
            
        return final_T
    
    def _evaluate_model(self, G, T, t):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        if self.params is None:
            return 0.0
            
        T_kelvin = T + 273.15
        
        if self.model_type == "avrami_saturation":
            f_max, K0, Q, n, alpha = self.params
            R = 8.314
            grain_effect = 1 + alpha * (G - 8)
            K = K0 * np.exp(-Q / (R * T_kelvin)) * grain_effect
            return f_max * (1 - np.exp(-K * (t ** n)))
        
        elif self.model_type == "power_law":
            A, B, C, D, E = self.params
            R = 8.314
            temp_effect = np.exp(C / (R * T_kelvin))
            time_effect = t ** D
            grain_effect = 1 + E * (G - 8)
            return A * temp_effect * time_effect * grain_effect + B
        
        elif self.model_type == "logistic":
            f_max, k, t0, alpha, beta = self.params
            R = 8.314
            temp_factor = np.exp(beta / (R * T_kelvin))
            grain_factor = 1 + alpha * (G - 8)
            rate = k * temp_factor * grain_factor
            return f_max / (1 + np.exp(-rate * (t - t0)))
        
        elif self.model_type == "ensemble":
            f_max, K0, Q, n, alpha, w, beta = self.params
            R = 8.314
            
            grain_effect_avrami = 1 + alpha * (G - 8)
            K_avrami = K0 * np.exp(-Q / (R * T_kelvin)) * grain_effect_avrami
            f_avrami = f_max * (1 - np.exp(-K_avrami * (t ** n)))
            
            temp_effect_power = np.exp(beta / (R * T_kelvin))
            f_power = w * temp_effect_power * (t ** 0.3) * (1 + 0.03 * (G - 8))
            
            return f_avrami + f_power
        
        return 0.0
    
    def calculate_validation_metrics(self, data):
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        if self.params is None:
            return None
        
        G = data['G'].values
        T = data['T'].values
        t = data['t'].values
        f_exp = data['f_exp (%)'].values
        
        f_pred = np.array([self._evaluate_model(g, temp, time) for g, temp, time in zip(G, T, t)])
        
        residuals = f_pred - f_exp
        relative_errors = (residuals / np.maximum(f_exp, 0.1)) * 100
        
        valid_mask = np.isfinite(relative_errors) & (f_exp > 0.1)
        f_exp_valid = f_exp[valid_mask]
        f_pred_valid = f_pred[valid_mask]
        residuals_valid = residuals[valid_mask]
        relative_errors_valid = relative_errors[valid_mask]
        
        if len(f_exp_valid) == 0:
            return None
            
        mae = np.mean(np.abs(residuals_valid))
        rmse = np.sqrt(mean_squared_error(f_exp_valid, f_pred_valid))
        mape = np.mean(np.abs(relative_errors_valid))
        r2 = r2_score(f_exp_valid, f_pred_valid)
        
        validation_results = {
            'data': data.copy(),
            'predictions': f_pred,
            'residuals': residuals,
            'relative_errors': relative_errors,
            'metrics': {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape,
                'R2': r2
            }
        }
        
        return validation_results

    def calculate_working_range_metrics(self, data):
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–±–æ—á–µ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 580-630¬∞C"""
        if self.params is None:
            return None
        
        working_data = data[(data['T'] >= 580) & (data['T'] <= 630)]
        
        if len(working_data) == 0:
            return None
            
        G = working_data['G'].values
        T = working_data['T'].values
        t = working_data['t'].values
        f_exp = working_data['f_exp (%)'].values
        
        f_pred = np.array([self._evaluate_model(g, temp, time) for g, temp, time in zip(G, T, t)])
        
        residuals = f_pred - f_exp
        relative_errors = (residuals / np.maximum(f_exp, 0.1)) * 100
        
        working_metrics = {
            'MAE': np.mean(np.abs(residuals)),
            'RMSE': np.sqrt(mean_squared_error(f_exp, f_pred)),
            'MAPE': np.mean(np.abs(relative_errors)),
            'R2': r2_score(f_exp, f_pred),
            'MaxError': np.max(np.abs(residuals)),
            'DataPoints': len(working_data)
        }
        
        return working_metrics

def read_uploaded_file(uploaded_file):
    """–ß—Ç–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ
        if uploaded_file.name.endswith('.csv'):
            try:
                data = pd.read_csv(uploaded_file, decimal=',', encoding='utf-8')
            except:
                try:
                    data = pd.read_csv(uploaded_file, decimal=',', encoding='cp1251')
                except:
                    data = pd.read_csv(uploaded_file, decimal='.', encoding='utf-8')
        else:
            try:
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π —Ñ–∞–π–ª
                if uploaded_file.name.endswith('.xlsx'):
                    data = pd.read_excel(uploaded_file, engine='openpyxl')
                else:
                    data = pd.read_excel(uploaded_file, engine='xlrd')
                
                # –°–ù–ê–ß–ê–õ–ê –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å
                if data is not None and len(data) > 0:
                    # –¢–ï–ü–ï–†–¨ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
                    data_normalized = DataValidator.normalize_column_names(data)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    if not all(col in data_normalized.columns for col in ['G', 'T', 't', 'f_exp (%)']):
                        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ —Å–ª–æ–∂–Ω—ã–π —Ñ–∞–π–ª
                        st.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–ª–æ–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä...")
                        data = ComplexDataParser.extract_all_data(uploaded_file)
                    else:
                        data = data_normalized
                else:
                    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ, –ø—Ä–æ–±—É–µ–º —Å–ª–æ–∂–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
                    st.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å. –ü—Ä–æ–±—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä...")
                    data = ComplexDataParser.extract_all_data(uploaded_file)
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å: {e}. –ü—Ä–æ–±—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä...")
                data = ComplexDataParser.extract_all_data(uploaded_file)
        
        return data
        
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return None

def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = None
    if 'current_data' not in st.session_state:
        st.session_state.current_data = None
    if 'validation_results' not in st.session_state:
        st.session_state.validation_results = None
    if 'excluded_points' not in st.session_state:
        st.session_state.excluded_points = set()
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = "ensemble"
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–æ–∫
    tab1, tab2, tab3 = st.tabs(["üìä –î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª—å", "üßÆ –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä", "üìà –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"])
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    st.sidebar.header("üéØ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
    model_type = st.sidebar.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å",
        ["avrami_saturation", "power_law", "logistic", "ensemble"],
        format_func=lambda x: {
            "avrami_saturation": "–ê–≤—Ä–∞–º–∏ —Å –Ω–∞—Å—ã—â–µ–Ω–∏–µ–º",
            "power_law": "–°—Ç–µ–ø–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å",
            "logistic": "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å", 
            "ensemble": "–ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å"
        }[x],
        key="model_selector"
    )
    
    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    sample_data = pd.DataFrame({
        'G': [8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],
        'T': [600, 600, 600, 600, 650, 650, 650, 650, 600, 600, 600, 600, 650, 650, 650, 650, 600, 600, 600, 600, 650, 650, 650, 650, 700, 700],
        't': [2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000],
        'f_exp (%)': [1.76, 0.68, 0.94, 1.09, 0.67, 1.2, 1.48, 1.13, 0.87, 1.28, 2.83, 3.25, 1.88, 2.29, 3.25, 2.89, 1.261, 2.04, 2.38, 3.3, 3.2, 4.26, 5.069, 5.41, 3.3, 5.0]
    })
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    st.sidebar.header("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    
    uploaded_file = st.sidebar.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏",
        type=['csv', 'xlsx', 'xls']
    )
    
    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–ª–æ–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if uploaded_file is not None and uploaded_file.name.endswith(('.xlsx', '.xls')):
        if st.sidebar.button("üîß –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ª–æ–∂–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"):
            with st.spinner("–ü–∞—Ä—Å–∏–º —Å–ª–æ–∂–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞..."):
                data = ComplexDataParser.extract_all_data(uploaded_file)
                if data is not None and len(data) > 0:
                    data = DataValidator.normalize_column_names(data)
                    is_valid, message = DataValidator.validate_data(data)
                    if is_valid:
                        data['f_exp (%)'] = data['f_exp (%)'].round(3)
                        st.session_state.current_data = data
                        st.sidebar.success(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π!")
                        st.rerun()
                else:
                    st.sidebar.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞")

    # –û–°–ù–û–í–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê
    if uploaded_file is not None:
        data = read_uploaded_file(uploaded_file)
        if data is not None and len(data) > 0:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –ï–°–õ–ò –µ—â–µ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
            if 'G' not in data.columns or 'T' not in data.columns or 't' not in data.columns or 'f_exp (%)' not in data.columns:
                data = DataValidator.normalize_column_names(data)
            
            is_valid, message = DataValidator.validate_data(data)
            if is_valid:
                data['f_exp (%)'] = data['f_exp (%)'].round(3)
                st.session_state.current_data = data
                st.sidebar.success("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
            else:
                st.sidebar.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {message}")
        else:
            st.sidebar.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞")

    if st.session_state.current_data is None:
        st.session_state.current_data = sample_data

    # –í–ö–õ–ê–î–ö–ê 1: –î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª—å
    with tab1:
        st.header("üìä –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏")
        
        st.info("üí° **–°–Ω–∏–º–∏—Ç–µ –≥–∞–ª–æ—á–∫–∏ —Å —Ç–æ—á–µ–∫, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏—Ç–µ –∏—Å–∫–ª—é—á–∏—Ç—å –∏–∑ –∞–Ω–∞–ª–∏–∑–∞**")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö —Å —á–µ–∫–±–æ–∫—Å–∞–º–∏
        display_data = st.session_state.current_data.copy()
        display_data['‚Ññ'] = range(1, len(display_data) + 1)
        display_data['–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å'] = [i not in st.session_state.excluded_points for i in range(len(display_data))]
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —á–µ–∫–±–æ–∫—Å–∞–º–∏
        edited_df = st.data_editor(
            display_data,
            column_config={
                "‚Ññ": st.column_config.NumberColumn(width="small"),
                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å": st.column_config.CheckboxColumn(
                    width="small",
                    help="–°–Ω–∏–º–∏—Ç–µ –≥–∞–ª–æ—á–∫—É —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å —Ç–æ—á–∫—É"
                ),
                "G": st.column_config.NumberColumn(width="small"),
                "T": st.column_config.NumberColumn(width="small"),
                "t": st.column_config.NumberColumn(width="small"),
                "f_exp (%)": st.column_config.NumberColumn(format="%.3f", width="small")
            },
            column_order=["‚Ññ", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", "G", "T", "t", "f_exp (%)"],
            use_container_width=True,
            height=400
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫
        new_excluded = set()
        for i, used in enumerate(edited_df['–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å']):
            if not used:
                new_excluded.add(i)
        
        if new_excluded != st.session_state.excluded_points:
            st.session_state.excluded_points = new_excluded
            st.session_state.analyzer = None
            st.session_state.validation_results = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total = len(display_data)
        excluded = len(st.session_state.excluded_points)
        included = total - excluded
        
        col1, col2, col3 = st.columns(3)
        col1.metric("–í—Å–µ–≥–æ —Ç–æ—á–µ–∫", total)
        col2.metric("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è", included, delta=f"-{excluded}" if excluded > 0 else None)
        col3.metric("–ò—Å–∫–ª—é—á–µ–Ω–æ", excluded)
        
        if excluded > 0:
            st.warning(f"–ò—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏: {[i+1 for i in sorted(st.session_state.excluded_points)]}")
            if st.button("üîÑ –í–∫–ª—é—á–∏—Ç—å –≤—Å–µ —Ç–æ—á–∫–∏"):
                st.session_state.excluded_points = set()
                st.rerun()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analysis_data = st.session_state.current_data.copy()
        if st.session_state.excluded_points:
            analysis_data = analysis_data.drop(list(st.session_state.excluded_points)).reset_index(drop=True)
        
        # –ü–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–∏
        st.header("üéØ –ü–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏")
        
        model_names = {
            'avrami_saturation': '–ê–≤—Ä–∞–º–∏ —Å –Ω–∞—Å—ã—â–µ–Ω–∏–µ–º', 
            'power_law': '–°—Ç–µ–ø–µ–Ω–Ω–∞—è', 
            'logistic': '–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è', 
            'ensemble': '–ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è'
        }
        st.write(f"**–í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å:** {model_names[model_type]}")
        
        if st.button("üöÄ –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", type="primary", use_container_width=True):
            if len(analysis_data) < 5:
                st.error("‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ù—É–∂–Ω–æ –∫–∞–∫ –º–∏–Ω–∏–º—É–º 5 —Ç–æ—á–µ–∫.")
            else:
                analyzer = SigmaPhaseAnalyzer()
                with st.spinner("–ü–æ–¥–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏..."):
                    success = analyzer.fit_model(analysis_data, model_type)
                
                if success:
                    st.session_state.analyzer = analyzer
                    validation_results = analyzer.calculate_validation_metrics(analysis_data)
                    st.session_state.validation_results = validation_results
                    st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞! R¬≤ = {analyzer.R2:.4f}")
                    st.rerun()
        
        # –ü–æ–∫–∞–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if st.session_state.analyzer is not None:
            analyzer = st.session_state.analyzer
            
            st.subheader("üìà –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
            if analyzer.model_type == "ensemble":
                f_max, K0, Q, n, alpha, w, beta = analyzer.params
                cols = st.columns(4)
                cols[0].metric("f_max", f"{f_max:.3f}%")
                cols[1].metric("K‚ÇÄ", f"{K0:.2e}")
                cols[2].metric("Q", f"{Q/1000:.1f} –∫–î–∂/–º–æ–ª—å")
                cols[3].metric("n", f"{n:.3f}")
                cols[0].metric("Œ±", f"{alpha:.3f}")
                cols[1].metric("w", f"{w:.3f}")
                cols[2].metric("Œ≤", f"{beta:.0f}")
            
            st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
            col1, col2 = st.columns(2)
            col1.metric("R¬≤", f"{analyzer.R2:.4f}")
            col2.metric("RMSE", f"{analyzer.rmse:.3f}%")
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–±–æ—á–µ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
            st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ –≤ —Ä–∞–±–æ—á–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (580-630¬∞C)")
            working_metrics = analyzer.calculate_working_range_metrics(analysis_data)
            
            if working_metrics:
                cols = st.columns(3)
                cols[0].metric("–¢–æ—á–µ–∫ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ", working_metrics['DataPoints'])
                cols[1].metric("R¬≤ —Ä–∞–±–æ—á.", f"{working_metrics['R2']:.4f}")
                cols[2].metric("RMSE —Ä–∞–±–æ—á.", f"{working_metrics['RMSE']:.3f}%")
            else:
                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–±–æ—á–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ 580-630¬∞C –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫")
            
            st.subheader("üßÆ –§–æ—Ä–º—É–ª–∞ –º–æ–¥–µ–ª–∏")
            st.markdown(analyzer.final_formula)

    # –í–ö–õ–ê–î–ö–ê 2: –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä
    with tab2:
        st.header("üßÆ –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã")
        
        if st.session_state.analyzer is not None:
            analyzer = st.session_state.analyzer
            
            col1, col2, col3 = st.columns(3)
            with col1:
                G_input = st.number_input("–ù–æ–º–µ—Ä –∑–µ—Ä–Ω–∞ (G)", value=8.0, min_value=-3.0, max_value=14.0, step=0.1)
            with col2:
                sigma_input = st.number_input("–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–∏–≥–º–∞-—Ñ–∞–∑—ã (%)", value=2.0, min_value=0.1, max_value=20.0, step=0.1)
            with col3:
                t_input = st.number_input("–í—Ä–µ–º—è (—á)", value=4000, min_value=100, max_value=500000, step=100)
            
            if st.button("üîç –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É", use_container_width=True):
                try:
                    T_pred = analyzer.predict_temperature(G_input, sigma_input, t_input)
                    
                    # –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    if 580 <= T_pred <= 630:
                        st.success(f"**–†–∞—Å—á–µ—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏:** {T_pred:.1f}¬∞C")
                        st.info("‚úÖ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º —Ä–∞–±–æ—á–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ")
                    elif 550 <= T_pred < 580:
                        st.success(f"**–†–∞—Å—á–µ—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏:** {T_pred:.1f}¬∞C")
                        st.warning("‚ö†Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –±–ª–∏–∑–∫–∞ –∫ –Ω–∏–∂–Ω–µ–º—É –ø—Ä–µ–¥–µ–ª—É –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–∏–≥–º–∞-—Ñ–∞–∑—ã")
                    else:
                        st.success(f"**–†–∞—Å—á–µ—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏:** {T_pred:.1f}¬∞C")
                        st.warning("‚ö†Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–Ω–µ —Ç–∏–ø–∏—á–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞")
                        
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞: {e}")
        else:
            st.info("üëÜ –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª—å'")

    # –í–ö–õ–ê–î–ö–ê 3: –í–∞–ª–∏–¥–∞—Ü–∏—è
    with tab3:
        st.header("üìà –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
        
        if st.session_state.analyzer is not None and st.session_state.validation_results is not None:
            analyzer = st.session_state.analyzer
            validation = st.session_state.validation_results
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            metrics = validation['metrics']
            st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
            cols = st.columns(4)
            cols[0].metric("R¬≤", f"{metrics['R2']:.4f}")
            cols[1].metric("MAE", f"{metrics['MAE']:.3f}%")
            cols[2].metric("RMSE", f"{metrics['RMSE']:.3f}%")
            cols[3].metric("MAPE", f"{metrics['MAPE']:.2f}%")
            
            # –ì—Ä–∞—Ñ–∏–∫
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=validation['data']['f_exp (%)'],
                y=validation['predictions'],
                mode='markers',
                name='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è',
                marker=dict(size=10, color='blue')
            ))
            max_val = max(validation['data']['f_exp (%)'].max(), validation['predictions'].max())
            fig.add_trace(go.Scatter(
                x=[0, max_val], y=[0, max_val],
                mode='lines',
                name='–ò–¥–µ–∞–ª—å–Ω–æ',
                line=dict(color='red', dash='dash')
            ))
            fig.update_layout(
                title='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ vs –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç',
                xaxis_title='–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç (%)',
                yaxis_title='–ú–æ–¥–µ–ª—å (%)'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # –¢–∞–±–ª–∏—Ü–∞
            st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ")
            comp_df = validation['data'].copy()
            comp_df['f_pred (%)'] = validation['predictions'].round(3)
            comp_df['–û—à–∏–±–∫–∞ (%)'] = validation['residuals'].round(3)
            comp_df['–û—Ç–Ω. –æ—à–∏–±–∫–∞ (%)'] = validation['relative_errors'].round(1)
            st.dataframe(comp_df, use_container_width=True)
            
        else:
            st.info("üëÜ –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª—å'")

if __name__ == "__main__":
    main()
