import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EllipticEnvelope
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import pickle
import json
import io
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏–≥–º–∞-—Ñ–∞–∑—ã",
    page_icon="üî¨",
    layout="wide"
)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("üî¨ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∏–Ω–µ—Ç–∏–∫–∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–∏–≥–º–∞-—Ñ–∞–∑—ã –≤ —Å—Ç–∞–ª–∏ 12–•18–ù12–¢")
st.markdown("""
### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Å–∏–≥–º–∞-—Ñ–∞–∑—ã, –≤—Ä–µ–º–µ–Ω–∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –∏ –Ω–æ–º–µ—Ä—É –∑–µ—Ä–Ω–∞
""")

class DataValidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def normalize_column_names(df):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        column_mapping = {
            '–ù–æ–º–µ—Ä_–∑–µ—Ä–Ω–∞': 'G', '–ù–æ–º–µ—Ä –∑–µ—Ä–Ω–∞': 'G', '–ó–µ—Ä–Ω–æ': 'G',
            '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞': 'T', '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞_C': 'T', '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ ¬∞C': 'T',
            '–í—Ä–µ–º—è': 't', '–í—Ä–µ–º—è_—á': 't', '–í—Ä–µ–º—è, —á': 't',
            '–°–∏–≥–º–∞_—Ñ–∞–∑–∞': 'f_exp (%)', '–°–∏–≥–º–∞-—Ñ–∞–∑–∞': 'f_exp (%)', 
            '–°–∏–≥–º–∞_—Ñ–∞–∑–∞_%': 'f_exp (%)', '–°–∏–≥–º–∞ —Ñ–∞–∑–∞': 'f_exp (%)',
            'Grain': 'G', 'Grain_number': 'G', 'Grain size': 'G',
            'Temperature': 'T', 'Temp': 'T', 'Temperature_C': 'T',
            'Time': 't', 'Time_h': 't', 'Hours': 't',
            'Sigma_phase': 'f_exp (%)', 'Sigma': 'f_exp (%)', 
            'Sigma_%': 'f_exp (%)', 'f_exp': 'f_exp (%)'
        }
        
        df_normalized = df.copy()
        new_columns = {}
        
        for col in df.columns:
            col_clean = str(col).strip()
            if col_clean in column_mapping:
                new_columns[col] = column_mapping[col_clean]
            else:
                new_columns[col] = col_clean
        
        df_normalized.columns = [new_columns[col] for col in df.columns]
        return df_normalized
    
    @staticmethod
    def validate_data(df):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö"""
        required_columns = ['G', 'T', 't', 'f_exp (%)']
        
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return False, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"
        
        try:
            df['G'] = pd.to_numeric(df['G'], errors='coerce')
            df['T'] = pd.to_numeric(df['T'], errors='coerce')
            df['t'] = pd.to_numeric(df['t'], errors='coerce')
            df['f_exp (%)'] = pd.to_numeric(df['f_exp (%)'], errors='coerce')
        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö: {e}"
        
        if df[required_columns].isna().any().any():
            return False, "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—É—Å—Ç—ã–µ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö"
        
        if (df['G'] < -3).any() or (df['G'] > 14).any():
            return False, "–ù–æ–º–µ—Ä –∑–µ—Ä–Ω–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç -3 –¥–æ 14"
        
        if (df['T'] < 500).any() or (df['T'] > 1000).any():
            st.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ typical –¥–∏–∞–ø–∞–∑–æ–Ω 500-1000¬∞C")
        
        if (df['f_exp (%)'] < 0).any() or (df['f_exp (%)'] > 50).any():
            st.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å–∏–≥–º–∞-—Ñ–∞–∑—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ typical –¥–∏–∞–ø–∞–∑–æ–Ω 0-50%")
        
        DataValidator.validate_time_range(df['t'])
        
        return True, "–î–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã"
    
    @staticmethod
    def validate_time_range(t_values):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏"""
        max_time = 500000
        if (t_values > max_time).any():
            st.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ —Å–≤—ã—à–µ {max_time} —á–∞—Å–æ–≤")
        return True

class AdvancedSigmaPhaseAnalyzer:
    def __init__(self):
        self.params = None
        self.R2 = None
        self.rmse = None
        self.mape = None
        self.model_type = None
        self.creation_date = datetime.now().isoformat()
        
    def fit_avrami_model(self, data):
        """–ú–æ–¥–µ–ª—å –ê–≤—Ä–∞–º–∏ —Å –Ω–∞—Å—ã—â–µ–Ω–∏–µ–º"""
        try:
            G = data['G'].values
            T = data['T'].values + 273.15
            t = data['t'].values
            f_exp = data['f_exp (%)'].values
            
            # –ù–∞—á–∞–ª—å–Ω—ã–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è
            # f_max, K0, Q, n, alpha
            initial_guess = [10.0, 1e10, 200000, 1.0, 0.1]
            bounds = (
                [1.0, 1e5, 100000, 0.1, -1.0],
                [20.0, 1e15, 400000, 3.0, 1.0]
            )
            
            def model(params, G, T, t):
                f_max, K0, Q, n, alpha = params
                R = 8.314
                
                # –í–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∑–µ—Ä–Ω–∞
                grain_effect = 1 + alpha * (G - 8)  # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –≤–æ–∫—Ä—É–≥ G=8
                
                # –ö–∏–Ω–µ—Ç–∏–∫–∞
                K = K0 * np.exp(-Q / (R * T)) * grain_effect
                f_pred = f_max * (1 - np.exp(-K * (t ** n)))
                return f_pred
            
            self.params, _ = curve_fit(
                lambda x, f_max, K0, Q, n, alpha: model([f_max, K0, Q, n, alpha], G, T, t),
                np.arange(len(G)), f_exp,
                p0=initial_guess,
                bounds=bounds,
                maxfev=5000
            )
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            f_pred = model(self.params, G, T, t)
            self.R2 = r2_score(f_exp, f_pred)
            self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
            self.mape = np.mean(np.abs((f_exp - f_pred) / f_exp)) * 100
            self.model_type = "avrami_saturation"
            
            return True
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ –ê–≤—Ä–∞–º–∏: {e}")
            return False
    
    def fit_power_law_model(self, data):
        """–°—Ç–µ–ø–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å—é"""
        try:
            G = data['G'].values
            T = data['T'].values + 273.15
            t = data['t'].values
            f_exp = data['f_exp (%)'].values
            
            # A, B, C, D, E - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            initial_guess = [1.0, 0.1, -10000, 0.5, 0.01]
            bounds = (
                [0.1, -1.0, -50000, 0.1, -0.1],
                [10.0, 1.0, -1000, 2.0, 0.1]
            )
            
            def model(params, G, T, t):
                A, B, C, D, E = params
                R = 8.314
                
                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
                temp_effect = np.exp(C / (R * T))
                time_effect = t ** D
                grain_effect = 1 + E * (G - 8)
                
                f_pred = A * temp_effect * time_effect * grain_effect + B
                return np.clip(f_pred, 0, 20)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            
            self.params, _ = curve_fit(
                lambda x, A, B, C, D, E: model([A, B, C, D, E], G, T, t),
                np.arange(len(G)), f_exp,
                p0=initial_guess,
                bounds=bounds,
                maxfev=5000
            )
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            f_pred = model(self.params, G, T, t)
            self.R2 = r2_score(f_exp, f_pred)
            self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
            self.mape = np.mean(np.abs((f_exp - f_pred) / f_exp)) * 100
            self.model_type = "power_law"
            
            return True
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤ —Å—Ç–µ–ø–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def fit_logistic_model(self, data):
        """–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å —Ä–æ—Å—Ç–∞"""
        try:
            G = data['G'].values
            T = data['T'].values + 273.15
            t = data['t'].values
            f_exp = data['f_exp (%)'].values
            
            # f_max, k, t0, alpha, beta
            initial_guess = [8.0, 1e-4, 1000, 0.1, -10000]
            bounds = (
                [1.0, 1e-8, 100, -1.0, -50000],
                [15.0, 1e-2, 10000, 1.0, -1000]
            )
            
            def model(params, G, T, t):
                f_max, k, t0, alpha, beta = params
                R = 8.314
                
                # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å–∫–æ—Ä–æ—Å—Ç–∏
                temp_factor = np.exp(beta / (R * T))
                grain_factor = 1 + alpha * (G - 8)
                
                # –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ä–æ—Å—Ç
                rate = k * temp_factor * grain_factor
                f_pred = f_max / (1 + np.exp(-rate * (t - t0)))
                return f_pred
            
            self.params, _ = curve_fit(
                lambda x, f_max, k, t0, alpha, beta: model([f_max, k, t0, alpha, beta], G, T, t),
                np.arange(len(G)), f_exp,
                p0=initial_guess,
                bounds=bounds,
                maxfev=5000
            )
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            f_pred = model(self.params, G, T, t)
            self.R2 = r2_score(f_exp, f_pred)
            self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
            self.mape = np.mean(np.abs((f_exp - f_pred) / f_exp)) * 100
            self.model_type = "logistic"
            
            return True
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def fit_ensemble_model(self, data):
        """–ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤"""
        try:
            G = data['G'].values
            T = data['T'].values + 273.15
            t = data['t'].values
            f_exp = data['f_exp (%)'].values
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
            initial_guess = [5.0, 1e8, 150000, 0.5, 0.1, 0.1, -20000]
            bounds = (
                [1.0, 1e5, 100000, 0.1, -1.0, 0.01, -50000],
                [15.0, 1e12, 300000, 2.0, 1.0, 1.0, -1000]
            )
            
            def model(params, G, T, t):
                f_max, K0, Q, n, alpha, w, beta = params
                R = 8.314
                
                # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –ê–≤—Ä–∞–º–∏
                grain_effect_avrami = 1 + alpha * (G - 8)
                K_avrami = K0 * np.exp(-Q / (R * T)) * grain_effect_avrami
                f_avrami = f_max * (1 - np.exp(-K_avrami * (t ** n)))
                
                # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç —Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞
                temp_effect_power = np.exp(beta / (R * T))
                f_power = w * temp_effect_power * (t ** 0.5) * (1 + 0.05 * (G - 8))
                
                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
                f_pred = f_avrami + f_power
                return np.clip(f_pred, 0, 15)
            
            self.params, _ = curve_fit(
                lambda x, f_max, K0, Q, n, alpha, w, beta: model([f_max, K0, Q, n, alpha, w, beta], G, T, t),
                np.arange(len(G)), f_exp,
                p0=initial_guess,
                bounds=bounds,
                maxfev=10000
            )
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            f_pred = model(self.params, G, T, t)
            self.R2 = r2_score(f_exp, f_pred)
            self.rmse = np.sqrt(mean_squared_error(f_exp, f_pred))
            self.mape = np.mean(np.abs((f_exp - f_pred) / f_exp)) * 100
            self.model_type = "ensemble"
            
            return True
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def predict_temperature(self, G, sigma_percent, t, method="bisection"):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        if self.params is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        sigma = sigma_percent
        
        if method == "bisection":
            return self._predict_temperature_bisection(G, sigma, t)
        else:
            return self._predict_temperature_analytic(G, sigma, t)
    
    def _predict_temperature_bisection(self, G, sigma, t, tol=1.0, max_iter=100):
        """–ë–∏—Å–µ–∫—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã"""
        T_min, T_max = 500, 900  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        
        for i in range(max_iter):
            T_mid = (T_min + T_max) / 2
            f_pred = self._evaluate_model(G, T_mid, t)
            
            if abs(f_pred - sigma) < tol:
                return T_mid
            
            if f_pred < sigma:
                T_min = T_mid
            else:
                T_max = T_mid
        
        return (T_min + T_max) / 2
    
    def _predict_temperature_analytic(self, G, sigma, t):
        """–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ (–≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ)"""
        if self.model_type == "avrami_saturation":
            f_max, K0, Q, n, alpha = self.params
            R = 8.314
            
            grain_effect = 1 + alpha * (G - 8)
            K_eff = K0 * grain_effect
            
            if sigma >= f_max or sigma <= 0:
                return None
            
            term = -np.log(1 - sigma / f_max) / (K_eff * (t ** n))
            if term <= 0:
                return None
            
            T_kelvin = -Q / (R * np.log(term))
            return T_kelvin - 273.15
        
        return self._predict_temperature_bisection(G, sigma, t)
    
    def _evaluate_model(self, G, T, t):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        T_kelvin = T + 273.15
        
        if self.model_type == "avrami_saturation":
            f_max, K0, Q, n, alpha = self.params
            R = 8.314
            grain_effect = 1 + alpha * (G - 8)
            K = K0 * np.exp(-Q / (R * T_kelvin)) * grain_effect
            return f_max * (1 - np.exp(-K * (t ** n)))
        
        elif self.model_type == "power_law":
            A, B, C, D, E = self.params
            R = 8.314
            temp_effect = np.exp(C / (R * T_kelvin))
            time_effect = t ** D
            grain_effect = 1 + E * (G - 8)
            return A * temp_effect * time_effect * grain_effect + B
        
        elif self.model_type == "logistic":
            f_max, k, t0, alpha, beta = self.params
            R = 8.314
            temp_factor = np.exp(beta / (R * T_kelvin))
            grain_factor = 1 + alpha * (G - 8)
            rate = k * temp_factor * grain_factor
            return f_max / (1 + np.exp(-rate * (t - t0)))
        
        elif self.model_type == "ensemble":
            f_max, K0, Q, n, alpha, w, beta = self.params
            R = 8.314
            
            # –ê–≤—Ä–∞–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
            grain_effect_avrami = 1 + alpha * (G - 8)
            K_avrami = K0 * np.exp(-Q / (R * T_kelvin)) * grain_effect_avrami
            f_avrami = f_max * (1 - np.exp(-K_avrami * (t ** n)))
            
            # –°—Ç–µ–ø–µ–Ω–Ω–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
            temp_effect_power = np.exp(beta / (R * T_kelvin))
            f_power = w * temp_effect_power * (t ** 0.5) * (1 + 0.05 * (G - 8))
            
            return f_avrami + f_power
        
        return 0.0
    
    def calculate_validation_metrics(self, data):
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        if self.params is None:
            return None
        
        G = data['G'].values
        T = data['T'].values
        t = data['t'].values
        f_exp = data['f_exp (%)'].values
        
        f_pred = np.array([self._evaluate_model(g, temp, time) for g, temp, time in zip(G, T, t)])
        
        residuals = f_pred - f_exp
        relative_errors = (residuals / f_exp) * 100
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        valid_mask = np.isfinite(relative_errors) & (f_exp > 0.1)
        f_exp_valid = f_exp[valid_mask]
        f_pred_valid = f_pred[valid_mask]
        residuals_valid = residuals[valid_mask]
        relative_errors_valid = relative_errors[valid_mask]
        
        mae = np.mean(np.abs(residuals_valid))
        rmse = np.sqrt(mean_squared_error(f_exp_valid, f_pred_valid))
        mape = np.mean(np.abs(relative_errors_valid))
        r2 = r2_score(f_exp_valid, f_pred_valid)
        
        validation_results = {
            'data': data.copy(),
            'predictions': f_pred,
            'residuals': residuals,
            'relative_errors': relative_errors,
            'metrics': {
                'MAE': mae,
                'RMSE': rmse,
                'MAPE': mape,
                'R2': r2
            }
        }
        
        return validation_results

def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = None
    if 'current_data' not in st.session_state:
        st.session_state.current_data = None
    if 'validation_results' not in st.session_state:
        st.session_state.validation_results = None
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    st.sidebar.header("üéØ –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏")
    
    model_type = st.sidebar.selectbox(
        "–¢–∏–ø —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏",
        ["avrami_saturation", "power_law", "logistic", "ensemble"],
        format_func=lambda x: {
            "avrami_saturation": "–ê–≤—Ä–∞–º–∏ —Å –Ω–∞—Å—ã—â–µ–Ω–∏–µ–º",
            "power_law": "–°—Ç–µ–ø–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å", 
            "logistic": "–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ä–æ—Å—Ç",
            "ensemble": "–ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å"
        }[x],
        help="–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å, –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –¥–ª—è –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö"
    )
    
    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    sample_data = pd.DataFrame({
        'G': [8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],
        'T': [600, 600, 600, 600, 650, 650, 650, 650, 600, 600, 600, 600, 650, 650, 650, 650, 600, 600, 600, 600, 650, 650, 650, 650, 700, 700],
        't': [2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000, 6000, 8000, 2000, 4000],
        'f_exp (%)': [1.76, 0.68, 0.94, 1.09, 0.67, 1.2, 1.48, 1.13, 0.87, 1.28, 2.83, 3.25, 1.88, 2.29, 3.25, 2.89, 1.261, 2.04, 2.38, 3.3, 3.2, 4.26, 5.069, 5.41, 3.3, 5.0]
    })
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    st.header("üîß –£–ª—É—á—à–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–≥–º–∞-—Ñ–∞–∑—ã")
    
    st.info("""
    **–ü—Ä–æ–±–ª–µ–º–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π:** –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–µ –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (MAPE > 40%)
    
    **–ù–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã:**
    - –ú–æ–¥–µ–ª—å –ê–≤—Ä–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –Ω–∞—Å—ã—â–µ–Ω–∏—è
    - –°—Ç–µ–ø–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å—é  
    - –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å —Ä–æ—Å—Ç–∞
    - –ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    st.session_state.current_data = sample_data
    
    st.header("üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    st.dataframe(st.session_state.current_data, use_container_width=True)
    
    # –ü–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–∏
    st.header("üéØ –ü–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏")
    
    if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", use_container_width=True):
        analyzer = AdvancedSigmaPhaseAnalyzer()
        
        with st.spinner("–ü–æ–¥–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏..."):
            if model_type == "avrami_saturation":
                success = analyzer.fit_avrami_model(st.session_state.current_data)
            elif model_type == "power_law":
                success = analyzer.fit_power_law_model(st.session_state.current_data)
            elif model_type == "logistic":
                success = analyzer.fit_logistic_model(st.session_state.current_data)
            else:  # ensemble
                success = analyzer.fit_ensemble_model(st.session_state.current_data)
        
        if success:
            st.session_state.analyzer = analyzer
            validation_results = analyzer.calculate_validation_metrics(st.session_state.current_data)
            st.session_state.validation_results = validation_results
            
            st.success(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞! R¬≤ = {analyzer.R2:.4f}")
            
            # –ü–æ–∫–∞–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            st.subheader("üìà –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
            if analyzer.model_type == "avrami_saturation":
                f_max, K0, Q, n, alpha = analyzer.params
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1: st.metric("f_max", f"{f_max:.2f}%")
                with col2: st.metric("K‚ÇÄ", f"{K0:.2e}")
                with col3: st.metric("Q", f"{Q/1000:.1f} –∫–î–∂/–º–æ–ª—å")
                with col4: st.metric("n", f"{n:.3f}")
                with col5: st.metric("Œ±", f"{alpha:.3f}")
                
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    if st.session_state.validation_results is not None:
        st.header("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        
        validation = st.session_state.validation_results
        metrics = validation['metrics']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1: st.metric("R¬≤", f"{metrics['R2']:.4f}")
        with col2: st.metric("MAE", f"{metrics['MAE']:.3f}%")
        with col3: st.metric("RMSE", f"{metrics['RMSE']:.3f}%")
        with col4: st.metric("MAPE", f"{metrics['MAPE']:.2f}%")
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if metrics['MAPE'] < 15:
            st.success("‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏!")
        elif metrics['MAPE'] < 25:
            st.warning("‚ö†Ô∏è –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏")
        else:
            st.error("‚ùå –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å")
            
        # –ì—Ä–∞—Ñ–∏–∫
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=validation['data']['f_exp (%)'],
            y=validation['predictions'],
            mode='markers',
            name='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è',
            marker=dict(size=10, color='blue')
        ))
        fig.add_trace(go.Scatter(
            x=[0, 6], y=[0, 6],
            mode='lines',
            name='–ò–¥–µ–∞–ª—å–Ω–æ',
            line=dict(color='red', dash='dash')
        ))
        fig.update_layout(
            title='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ vs –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç',
            xaxis_title='–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (%)',
            yaxis_title='–†–∞—Å—á–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (%)'
        )
        st.plotly_chart(fig, use_container_width=True)

if __name__ == "__main__":
    main()
